{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Imports and importing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nfVYNpQ6Hkt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_squared_error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "v9NPfOkU6Q3r",
        "outputId": "1ceeac36-b90e-4c88-b9cd-4d8cf4429fd5"
      },
      "outputs": [],
      "source": [
        "rateMyProfData = pd.read_csv('Cleaned_UW_RMP.csv')\n",
        "rateMyProfData.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Data Exploration**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Further Data Checking and Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# See the null values by each row\n",
        "rateMyProfData.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the above cell, we can see there are a significant number of null values in the columns that are optional for users when leaving a review. Due to the sheer number of null values, This information is not neccessary or useful. We will drop these for the rest of our analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rateMyProfData.drop(columns=['For-Credit', \n",
        "                         'Attendance', \n",
        "                         'Take-Again', \n",
        "                         'Grade', \n",
        "                         'Textbook'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Further we need to deal with the null values that are still in place. For the review body, there is only one null value, and the review body is necessary so we will drop this row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rateMyProfData = rateMyProfData.dropna(subset=['Review-Body'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And finally, we chose to drop rows with null values course name and number, to keep the data from those for some interesting analysis later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rateMyProfData = rateMyProfData.dropna(subset=['Course-Name', 'Course-Number'])\n",
        "rateMyProfData['Course-Number'] = [str(num)[0] for num in rateMyProfData['Course-Number']] \n",
        "# user input data is all over the place, \n",
        "# take the first digit for the class year, ie 1 = 100 level, 4 = 400 level.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Data Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "DyHq1bYa7C_g",
        "outputId": "69f229ab-8926-4ce4-bbb9-65df2a4461f9"
      },
      "outputs": [],
      "source": [
        "rateMyProfData['Quality'].plot(kind='hist', bins=9, title='Quality', align='mid', width=0.4)\n",
        "\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)\n",
        "plt.xlabel('Quality')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "Y_NDSKKL8MZj",
        "outputId": "1f6ff970-889b-4523-c55c-3b44f56af047"
      },
      "outputs": [],
      "source": [
        "rateMyProfData['Difficulty'].plot(kind='hist', bins=5, title='Difficulty', width=0.6)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)\n",
        "plt.xticks(range(1, 6))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rateMyProfData['Course-Number'].value_counts().plot(kind='bar')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "for i, course_number in enumerate([1, 2, 3, 4]):\n",
        "  ax = axes[i // 2, i % 2]\n",
        "  course_data = rateMyProfData[rateMyProfData['Course-Number'] == str(course_number)]\n",
        "  course_data['Quality'].plot(kind='hist', bins=9, title=f'Quality Distribution for {course_number}00 level courses', ax=ax, width=0.4)\n",
        "  ax.spines[['top', 'right']].set_visible(False)\n",
        "  ax.set_xlabel('Quality')\n",
        "  ax.set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "for i, course_number in enumerate([1, 2, 3, 4]):\n",
        "    ax = axes[i // 2, i % 2]\n",
        "    course_data = rateMyProfData[rateMyProfData['Course-Number'] == str(course_number)]\n",
        "    difficulty_values = course_data['Difficulty']\n",
        "    bin_edges = np.arange(0.5, 6, 1)  # Calculate bin edges manually to center bars\n",
        "    ax.hist(difficulty_values, bins=bin_edges, align='mid', rwidth=0.8)\n",
        "    ax.set_title(f'Difficulty Distribution for {course_number}00 level courses')\n",
        "    ax.set_xlabel('Difficulty')\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.set_xticks(range(1, 6))\n",
        "    ax.set_xticklabels(range(1, 6))\n",
        "    ax.spines[['top', 'right']].set_visible(False)\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "course_counts = rateMyProfData['Course-Name'].value_counts()\n",
        "for course_name, count in course_counts.items():\n",
        "  if count > 100:\n",
        "    print(f\"{course_name}: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(3, 3, figsize=(12, 10))\n",
        "\n",
        "for i, course_name in enumerate(course_counts.index[:9]):\n",
        "  ax = axes[i // 3, i % 3]\n",
        "  course_data = rateMyProfData[rateMyProfData['Course-Name'] == course_name]\n",
        "  course_data['Quality'].plot(kind='hist', bins=9, title=f'{course_name} Quality', ax=ax, width=0.4)\n",
        "  ax.spines[['top', 'right']].set_visible(False)\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(3, 3, figsize=(12, 10))\n",
        "\n",
        "for i, course_name in enumerate(course_counts.index[:9]):\n",
        "  ax = axes[i // 3, i % 3]\n",
        "  course_data = rateMyProfData[rateMyProfData['Course-Name'] == course_name]\n",
        "  bin_edges = np.arange(0.5, 6, 1)  # Calculate bin edges manually to center bars\n",
        "  course_data['Difficulty'].plot(kind='hist', bins=bin_edges, title=f'{course_name} Difficulty', ax=ax, width=0.6)\n",
        "  ax.spines[['top', 'right']].set_visible(False)\n",
        "  ax.set_xlabel('Difficulty')\n",
        "  ax.set_ylabel('Frequency')\n",
        "  ax.set_xticks(range(1, 6))\n",
        "  ax.set_xticklabels(range(1, 6))\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Possible analysis with Course name, date, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Prediction of Quality and Difficulty**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Basic Machine Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To start creating a prediction of quality and difficulty for the reviews, we need to do some pre-processing on the review bodies. Our goal is to compare a basic Machine Learning model here to a more robust transformer. We have chosen to use a linear regression model, and to compare it to a BERT model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Review Pre-Processing for Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Send to lowercase\n",
        "rateMyProfData['cleanReview'] = rateMyProfData['Review-Body'].str.lower()\n",
        "# remove numbers\n",
        "rateMyProfData['cleanReview'] = rateMyProfData['cleanReview'].apply(lambda x: re.sub(r'\\d+', '', x)) \n",
        "# remove punctuation\n",
        "rateMyProfData['cleanReview'] = rateMyProfData['cleanReview'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation))) \n",
        "# remove extra spaces\n",
        "rateMyProfData['cleanReview'] = rateMyProfData['cleanReview'].apply(lambda x: ' '.join([token for token in x.split()]))\n",
        "# remove stop words\n",
        "stop = set(stopwords.words('english'))\n",
        "rateMyProfData['cleanReview'] = rateMyProfData['cleanReview'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "\n",
        "rateMyProfData['cleanReview'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Word Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rateMyProfData['review_tokens'] = rateMyProfData['cleanReview'].apply(lambda x: word_tokenize(x))\n",
        "rateMyProfData['review_tokens'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### POS Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rateMyProfData['review_tokens'] = rateMyProfData['review_tokens'].apply(lambda x: nltk.pos_tag(x))\n",
        "rateMyProfData['review_tokens'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Lemmatizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# need this to get correct POS tag for nltk lemmatizer\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "rateMyProfData['review_tokens'] = rateMyProfData['review_tokens'].apply(lambda x: [lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
        "                                                                        if get_wordnet_pos(tag)\n",
        "                                                                        else lemmatizer.lemmatize(word)\n",
        "                                                                        for word, tag in x])\n",
        "rateMyProfData['review_tokens'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Change it to a list of strings so BoW can operate correctly\n",
        "rateMyProfData['cleanReview'] = rateMyProfData['review_tokens'].apply(lambda x: ' '.join(x))\n",
        "rateMyProfData['cleanReview'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating the Linear Regression Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split the data into test and train for both predictions\n",
        "X_train_Qual, X_test_Qual, y_train_Qual, y_test_Qual = train_test_split(rateMyProfData['cleanReview'], \n",
        "                                                                        rateMyProfData['Quality'], \n",
        "                                                                        test_size=0.2, \n",
        "                                                                        random_state=42)\n",
        "X_train_Diff, X_test_Diff, y_train_Diff, y_test_Diff = train_test_split(rateMyProfData['cleanReview'], \n",
        "                                                                        rateMyProfData['Difficulty'], \n",
        "                                                                        test_size=0.2, \n",
        "                                                                        random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the bodies for analysis later\n",
        "X_test_Qual_Orig = X_test_Qual\n",
        "X_test_Diff_Orig = X_test_Diff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer_Qual = CountVectorizer()\n",
        "regressor_Qual = LinearRegression()\n",
        "\n",
        "vectorizer_Diff = CountVectorizer()\n",
        "regressor_Diff = LinearRegression()\n",
        "\n",
        "# Convert the text to a bag-of-words representation\n",
        "X_train_Qual = vectorizer_Qual.fit_transform(X_train_Qual)\n",
        "X_test_Qual = vectorizer_Qual.transform(X_test_Qual)\n",
        "\n",
        "regressor_Qual.fit(X_train_Qual, y_train_Qual)\n",
        "y_pred_Qual = regressor_Qual.predict(X_test_Qual)\n",
        "\n",
        "X_train_Diff = vectorizer_Diff.fit_transform(X_train_Diff)\n",
        "X_test_Diff = vectorizer_Diff.transform(X_test_Diff)\n",
        "\n",
        "regressor_Diff.fit(X_train_Diff, y_train_Diff)\n",
        "y_pred_Diff = regressor_Diff.predict(X_test_Diff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyzing the Linear Regression performances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Quality Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mse = mean_squared_error(y_test_Qual, y_pred_Qual)\n",
        "print(\"Quality Mean Squared Error:\", mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "example_index = 1\n",
        "specific_instance = X_test_Qual[example_index]\n",
        "prediction = regressor_Qual.predict(specific_instance)\n",
        "actual_label = y_test_Qual.iloc[example_index]\n",
        "print(\"Cleaned review body at index:\", example_index)\n",
        "print(X_test_Qual_Orig.iloc[example_index])\n",
        "print(\"Actual X_test_Qual[2] label:\", actual_label)\n",
        "print(\"Prediction for X_test_Qual[2]:\", prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "example_index = 4\n",
        "specific_instance = X_test_Qual[example_index]\n",
        "prediction = regressor_Qual.predict(specific_instance)\n",
        "actual_label = y_test_Qual.iloc[example_index]\n",
        "print(\"Cleaned review body at index:\", example_index)\n",
        "print(X_test_Qual_Orig.iloc[example_index])\n",
        "print(\"Actual X_test_Qual[2] label:\", actual_label)\n",
        "print(\"Prediction for X_test_Qual[2]:\", prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, the performance is quite terrible. For a range of labels from 0.5 - 5.0, a mean squared error of almost 47 is very bad. \n",
        "\n",
        "Further the prediction at index 2 above, isnt too far off, but the prediction at index 4 is way out of scope. We can see from the review bodies that the review at index 4 contains very positive words, it is likely that the regressor is seeing a highly positive review and correlating a little too heavily, also not realizing that the max value is 5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One thing we found upon researching is to clip the prediction to between 0.5 and 5.0 for quality to eliminate over-estimating highly praising reviews. While this does not actually help the Machine Learning model, it does help the usefulness of the prediction, and the mean-squared error. It is much better now at 4.6, but 4.6 is still quite bad for a range of possible values between 0.5 and 5.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_Qual = np.clip(y_pred_Qual, 0.5, 5.0)\n",
        "\n",
        "mse = mean_squared_error(y_test_Qual, y_pred_Qual)\n",
        "print(\"Quality Mean Squared Error after clipping:\", mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Difficulty Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mse = mean_squared_error(y_test_Diff, y_pred_Diff)\n",
        "print(\"Difficulty Mean Squared Error:\", mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "example_index = 7\n",
        "specific_instance = X_test_Diff[example_index]\n",
        "prediction = regressor_Diff.predict(specific_instance)\n",
        "actual_label = y_test_Diff.iloc[example_index]\n",
        "print(\"Cleaned review body at index:\", example_index)\n",
        "print(X_test_Diff_Orig.iloc[example_index])\n",
        "print(\"Actual X_test_Diff[2] label:\", actual_label)\n",
        "print(\"Prediction for X_test_Diff[2]:\", prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "example_index = 6\n",
        "specific_instance = X_test_Diff[example_index]\n",
        "prediction = regressor_Diff.predict(specific_instance)\n",
        "actual_label = y_test_Diff.iloc[example_index]\n",
        "print(\"Cleaned review body at index:\", example_index)\n",
        "print(X_test_Diff_Orig.iloc[example_index])\n",
        "print(\"Actual X_test_Diff[2] label:\", actual_label)\n",
        "print(\"Prediction for X_test_Diff[2]:\", prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Just as before with quality, the performance of the difficulty model is very similarly terrible. This model has a mean-squared error of about 46, which is slightly better. \n",
        "\n",
        "The prediction at index 7 above is pretty close, even though the word confuse is in the review, which we thought would increase the difficulty prediction. \n",
        "\n",
        "At index 6 also above is even more interesting, even though the review conatains negative phrasing such as dont ask, and contains the word hard, the model predictied a difficulty score of 1.5 relating to very easy, while the true label was 5 out of 5 difficulty. In this review, it is likely because the context of the word easy. In the review to a human reader it seems that the intent of the word easy here is the author contradicting what others have said, and disagreeing with it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clipping the prediction to between 1 and 5 for quality to eliminate out of bounds reviews helped here again. Similarly, it does not help the prediction model, but just the output and mean-squared error. It is much better now at 3.7, but again 3.7 is still quite bad for a range of possible values between 1 and 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_Diff = np.clip(y_pred_Diff, 1, 5)\n",
        "\n",
        "mse = mean_squared_error(y_test_Diff, y_pred_Diff)\n",
        "print(\"Difficulty Mean Squared Error after clipping:\", mse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **BERT NLP Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
